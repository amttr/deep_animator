{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "\n",
    "> Network architecture for the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from deep_animator.modules.dense_motion import DenseMotionNetwork\n",
    "from deep_animator.modules.utils import SameBlock2d, DownBlock2d, UpBlock2d, ResBlock2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class OcclusionAwareGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Transforms a source image according to movement trajectories induced by keypoints. \n",
    "    Generator follows Johnson architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, num_kp, block_expansion, max_features,\n",
    "                 num_down_blocks, num_bottleneck_blocks, estimate_occlusion_map=False, \n",
    "                 dense_motion_params=None, estimate_jacobian=False):\n",
    "        super(OcclusionAwareGenerator, self).__init__()\n",
    "\n",
    "        if dense_motion_params is not None:\n",
    "            self.dense_motion_network = DenseMotionNetwork(num_kp=num_kp, num_channels=num_channels,\n",
    "                                                           estimate_occlusion_map=estimate_occlusion_map,\n",
    "                                                           **dense_motion_params)\n",
    "        else:\n",
    "            self.dense_motion_network = None\n",
    "\n",
    "        self.first = SameBlock2d(num_channels, block_expansion, kernel_size=(7, 7), padding=(3, 3))\n",
    "\n",
    "        down_blocks = []\n",
    "        for i in range(num_down_blocks):\n",
    "            in_features = min(max_features, block_expansion * (2 ** i))\n",
    "            out_features = min(max_features, block_expansion * (2 ** (i + 1)))\n",
    "            down_blocks.append(DownBlock2d(in_features, out_features, kernel_size=(3, 3), padding=(1, 1)))\n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "\n",
    "        up_blocks = []\n",
    "        for i in range(num_down_blocks):\n",
    "            in_features = min(max_features, block_expansion * (2 ** (num_down_blocks - i)))\n",
    "            out_features = min(max_features, block_expansion * (2 ** (num_down_blocks - i - 1)))\n",
    "            up_blocks.append(UpBlock2d(in_features, out_features, kernel_size=(3, 3), padding=(1, 1)))\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "\n",
    "        self.bottleneck = torch.nn.Sequential()\n",
    "        in_features = min(max_features, block_expansion * (2 ** num_down_blocks))\n",
    "        for i in range(num_bottleneck_blocks):\n",
    "            self.bottleneck.add_module('r' + str(i), ResBlock2d(in_features, kernel_size=(3, 3), padding=(1, 1)))\n",
    "\n",
    "        self.final = nn.Conv2d(block_expansion, num_channels, kernel_size=(7, 7), padding=(3, 3))\n",
    "        self.estimate_occlusion_map = estimate_occlusion_map\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def deform_input(self, inp, deformation):\n",
    "        _, h_old, w_old, _ = deformation.shape\n",
    "        _, _, h, w = inp.shape\n",
    "        if h_old != h or w_old != w:\n",
    "            deformation = deformation.permute(0, 3, 1, 2)\n",
    "            deformation = F.interpolate(deformation, size=(h, w), mode='bilinear')\n",
    "            deformation = deformation.permute(0, 2, 3, 1)\n",
    "        return F.grid_sample(inp, deformation)\n",
    "\n",
    "    def forward(self, source_image, kp_driving, kp_source):\n",
    "        # Encoding (downsampling) part\n",
    "        out = self.first(source_image)\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            out = self.down_blocks[i](out)\n",
    "\n",
    "        # Transforming feature representation according to deformation and occlusion\n",
    "        output_dict = {}\n",
    "        if self.dense_motion_network is not None:\n",
    "            dense_motion = self.dense_motion_network(source_image=source_image, kp_driving=kp_driving,\n",
    "                                                     kp_source=kp_source)\n",
    "            output_dict['mask'] = dense_motion['mask']\n",
    "            output_dict['sparse_deformed'] = dense_motion['sparse_deformed']\n",
    "\n",
    "            if 'occlusion_map' in dense_motion:\n",
    "                occlusion_map = dense_motion['occlusion_map']\n",
    "                output_dict['occlusion_map'] = occlusion_map\n",
    "            else:\n",
    "                occlusion_map = None\n",
    "            deformation = dense_motion['deformation']\n",
    "            out = self.deform_input(out, deformation)\n",
    "\n",
    "            if occlusion_map is not None:\n",
    "                if out.shape[2] != occlusion_map.shape[2] or out.shape[3] != occlusion_map.shape[3]:\n",
    "                    occlusion_map = F.interpolate(occlusion_map, size=out.shape[2:], mode='bilinear')\n",
    "                out = out * occlusion_map\n",
    "\n",
    "            output_dict[\"deformed\"] = self.deform_input(source_image, deformation)\n",
    "\n",
    "        # Decoding part\n",
    "        out = self.bottleneck(out)\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            out = self.up_blocks[i](out)\n",
    "        out = self.final(out)\n",
    "        out = F.sigmoid(out)\n",
    "\n",
    "        output_dict[\"prediction\"] = out\n",
    "\n",
    "        return output_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_animator]",
   "language": "python",
   "name": "conda-env-deep_animator-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
