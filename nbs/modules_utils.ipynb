{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Utils\n",
    "\n",
    "> Common network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from deep_animator.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d as BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DownBlock2d(nn.Module):\n",
    "    \"\"\"Downsampling block for use in encoder.\"\"\"\n",
    "    def __init__(self, in_features, out_features, kernel_size=3, padding=1, groups=1):\n",
    "        super(DownBlock2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_features, out_channels=out_features, \n",
    "                              kernel_size=kernel_size, padding=padding, groups=groups)\n",
    "        self.norm = BatchNorm2d(out_features, affine=True)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UpBlock2d(nn.Module):\n",
    "    \"\"\"Upsampling block for use in decoder.\"\"\"\n",
    "    def __init__(self, in_features, out_features, kernel_size=3, padding=1, groups=1):\n",
    "        super(UpBlock2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_features, out_channels=out_features, \n",
    "                              kernel_size=kernel_size, padding=padding, groups=groups)\n",
    "        self.norm = BatchNorm2d(out_features, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.interpolate(x, scale_factor=2)\n",
    "        out = self.conv(out)\n",
    "        out = self.norm(out)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Hourglass Encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, block_expansion, in_features, num_blocks=3, max_features=256):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        down_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            down_blocks.append(DownBlock2d(in_features if i == 0 else min(max_features, block_expansion * (2 ** i)),\n",
    "                                           min(max_features, block_expansion * (2 ** (i + 1))),\n",
    "                                           kernel_size=3, padding=1))\n",
    "        self.down_blocks = nn.ModuleList(down_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [x]\n",
    "        for down_block in self.down_blocks:\n",
    "            outs.append(down_block(outs[-1]))\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Hourglass Decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, block_expansion, in_features, num_blocks=3, max_features=256):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        up_blocks = []\n",
    "        for i in range(num_blocks)[::-1]:\n",
    "            in_filters = (1 if i == num_blocks - 1 else 2) * min(max_features, block_expansion * (2 ** (i + 1)))\n",
    "            out_filters = min(max_features, block_expansion * (2 ** i))\n",
    "            up_blocks.append(UpBlock2d(in_filters, out_filters, kernel_size=3, padding=1))\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(up_blocks)\n",
    "        self.out_filters = block_expansion + in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.pop()\n",
    "        for up_block in self.up_blocks:\n",
    "            out = up_block(out)\n",
    "            skip = x.pop()\n",
    "            out = torch.cat([out, skip], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Hourglass(nn.Module):\n",
    "    \"\"\"Hourglass architecture.\"\"\"\n",
    "    def __init__(self, block_expansion, in_features, num_blocks=3, max_features=256):\n",
    "        super(Hourglass, self).__init__()\n",
    "        self.encoder = Encoder(block_expansion, in_features, num_blocks, max_features)\n",
    "        self.decoder = Decoder(block_expansion, in_features, num_blocks, max_features)\n",
    "        self.out_filters = self.decoder.out_filters\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AntiAliasInterpolation2d(nn.Module):\n",
    "    \"\"\"Band-limited downsampling, for better preservation of the input signal.\"\"\"\n",
    "    def __init__(self, channels, scale):\n",
    "        super(AntiAliasInterpolation2d, self).__init__()\n",
    "        \n",
    "        sigma = (1 / scale - 1) / 2\n",
    "        kernel_size = 2 * round(sigma * 4) + 1\n",
    "        self.ka = kernel_size // 2\n",
    "        self.kb = self.ka - 1 if kernel_size % 2 == 0 else self.ka\n",
    "\n",
    "        kernel_size = [kernel_size, kernel_size]\n",
    "        sigma = [sigma, sigma]\n",
    "        \n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [torch.arange(size, dtype=torch.float32) for size in kernel_size]\n",
    "        )\n",
    "        \n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= torch.exp(-(mgrid - mean) ** 2 / (2 * std ** 2))\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.scale == 1.0:\n",
    "            return input\n",
    "\n",
    "        out = F.pad(input, (self.ka, self.kb, self.ka, self.kb))\n",
    "        out = F.conv2d(out, weight=self.weight, groups=self.groups)\n",
    "        out = F.interpolate(out, scale_factor=(self.scale, self.scale))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_coordinate_grid(spatial_size, type):\n",
    "    \"\"\"Create a meshgrid [-1,1] x [-1,1] of given spatial_size.\"\"\"\n",
    "    h, w = spatial_size\n",
    "    x = torch.arange(w).type(type)\n",
    "    y = torch.arange(h).type(type)\n",
    "\n",
    "    x = (2 * (x / (w - 1)) - 1)\n",
    "    y = (2 * (y / (h - 1)) - 1)\n",
    "\n",
    "    yy = y.view(-1, 1).repeat(1, w)\n",
    "    xx = x.view(1, -1).repeat(h, 1)\n",
    "\n",
    "    meshed = torch.cat([xx.unsqueeze_(2), yy.unsqueeze_(2)], 2)\n",
    "\n",
    "    return meshed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def kp2gaussian(kp, spatial_size, kp_variance):\n",
    "    \"\"\"Transform a keypoint into gaussian like representation.\"\"\"\n",
    "    mean = kp['value']\n",
    "\n",
    "    coordinate_grid = make_coordinate_grid(spatial_size, mean.type())\n",
    "    number_of_leading_dimensions = len(mean.shape) - 1\n",
    "    shape = (1,) * number_of_leading_dimensions + coordinate_grid.shape\n",
    "    coordinate_grid = coordinate_grid.view(*shape)\n",
    "    repeats = mean.shape[:number_of_leading_dimensions] + (1, 1, 1)\n",
    "    coordinate_grid = coordinate_grid.repeat(*repeats)\n",
    "\n",
    "    # Preprocess kp shape\n",
    "    shape = mean.shape[:number_of_leading_dimensions] + (1, 1, 2)\n",
    "    mean = mean.view(*shape)\n",
    "\n",
    "    mean_sub = (coordinate_grid - mean)\n",
    "\n",
    "    out = torch.exp(-0.5 * (mean_sub ** 2).sum(-1) / kp_variance)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gaussian2kp(heatmap):\n",
    "    \"\"\" Extract the mean from a heatmap.\"\"\"\n",
    "    shape = heatmap.shape\n",
    "    heatmap = heatmap.unsqueeze(-1)\n",
    "    grid = make_coordinate_grid(shape[2:], heatmap.type()).unsqueeze_(0).unsqueeze_(0)\n",
    "    value = (heatmap * grid).sum(dim=(2, 3))\n",
    "    kp = {'value': value}\n",
    "\n",
    "    return kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SameBlock2d(nn.Module):\n",
    "    \"\"\"Simple block, preserve spatial resolution.\"\"\"\n",
    "    def __init__(self, in_features, out_features, groups=1, kernel_size=3, padding=1):\n",
    "        super(SameBlock2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_features, out_channels=out_features,\n",
    "                              kernel_size=kernel_size, padding=padding, groups=groups)\n",
    "        self.norm = BatchNorm2d(out_features, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.norm(out)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ResBlock2d(nn.Module):\n",
    "    \"\"\"Res block, preserve spatial resolution.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, kernel_size, padding):\n",
    "        super(ResBlock2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_features, out_channels=in_features, kernel_size=kernel_size,\n",
    "                               padding=padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_features, out_channels=in_features, kernel_size=kernel_size,\n",
    "                               padding=padding)\n",
    "        self.norm1 = BatchNorm2d(in_features, affine=True)\n",
    "        self.norm2 = BatchNorm2d(in_features, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.norm1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.norm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out += x\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_animator]",
   "language": "python",
   "name": "conda-env-deep_animator-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
