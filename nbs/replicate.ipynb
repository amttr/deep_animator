{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sync_batchnorm.replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate\n",
    "\n",
    "> Part of Synchronized-BatchNorm-PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.nn.parallel.data_parallel import DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CallbackContext(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def execute_replication_callbacks(modules):\n",
    "    \"\"\"\n",
    "    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "    \n",
    "    Note that, as all modules are isomorphism, we assign each sub-module with a context\n",
    "    (shared among multiple copies of this module on different devices).\n",
    "    Through this context, different copies can share some information.\n",
    "    \n",
    "    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n",
    "    of any slave copies.\n",
    "    \"\"\"\n",
    "    master_copy = modules[0]\n",
    "    nr_modules = len(list(master_copy.modules()))\n",
    "    ctxs = [CallbackContext() for _ in range(nr_modules)]\n",
    "\n",
    "    for i, module in enumerate(modules):\n",
    "        for j, m in enumerate(module.modules()):\n",
    "            if hasattr(m, '__data_parallel_replicate__'):\n",
    "                m.__data_parallel_replicate__(ctxs[j], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataParallelWithCallback(DataParallel):\n",
    "    \"\"\"\n",
    "    Data Parallel with a replication callback.\n",
    "    \n",
    "    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n",
    "    original `replicate` function.\n",
    "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
    "    \n",
    "    Examples:\n",
    "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
    "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
    "        # sync_bn.__data_parallel_replicate__ will be invoked.\n",
    "    \"\"\"\n",
    "\n",
    "    def replicate(self, module, device_ids):\n",
    "        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n",
    "        execute_replication_callbacks(modules)\n",
    "        return modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_animator]",
   "language": "python",
   "name": "conda-env-deep_animator-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
