---

title: Batchnorm

keywords: fastai
sidebar: home_sidebar

summary: "Part of Synchronized-BatchNorm-PyTorch."
description: "Part of Synchronized-BatchNorm-PyTorch."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/batchnorm.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SynchronizedBatchNorm2d" class="doc_header"><code>class</code> <code>SynchronizedBatchNorm2d</code><a href="https://github.com/dpoulopoulos/deep_animator/tree/master/deep_animator/sync_batchnorm/batchnorm.py#L111" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SynchronizedBatchNorm2d</code>(<strong><code>num_features</code></strong>, <strong><code>eps</code></strong>=<em><code>1e-05</code></em>, <strong><code>momentum</code></strong>=<em><code>0.1</code></em>, <strong><code>affine</code></strong>=<em><code>True</code></em>) :: <code>_SynchronizedBatchNorm</code></p>
</blockquote>
<p>Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<p>.. math::
    y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta</p>
<p>This module differs from the built-in PyTorch BatchNorm2d as the mean and
standard-deviation are reduced across all devices during training.
For example, when one uses <code>nn.DataParallel</code> to wrap the network during
training, PyTorch's implementation normalize the tensor on each device using
the statistics only on that device, which accelerated the computation and
is also easy to implement, but the statistics might be inaccurate.
Instead, in this synchronized version, the statistics will be computed
over all training samples distributed on multiple devices.</p>
<p>Note that, for one-GPU or CPU-only case, this module behaves exactly same
as the built-in PyTorch implementation.
The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).
During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.
During evaluation, this running mean/variance is used for normalization.
Because the BatchNorm is done over the <code>C</code> dimension, computing statistics
on <code>(N, H, W)</code> slices, it's common terminology to call this Spatial BatchNorm</p>
<p>Args:
    num_features: num_features from an expected input of
        size batch_size x num_features x height x width
    eps: a value added to the denominator for numerical stability.
        Default: 1e-5
    momentum: the value used for the running_mean and running_var
        computation. Default: 0.1
    affine: a boolean value that when set to <code>True</code>, gives the layer learnable
        affine parameters. Default: <code>True</code>
Shape:</p>

<pre><code>- Input: :math:`(N, C, H, W)`
- Output: :math:`(N, C, H, W)` (same shape as input)

</code></pre>
<p>Examples:</p>

<pre><code>&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = SynchronizedBatchNorm2d(100)
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = SynchronizedBatchNorm2d(100, affine=False)
&gt;&gt;&gt; input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))
&gt;&gt;&gt; output = m(input)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

